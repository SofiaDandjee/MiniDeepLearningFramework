{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "torch.set_grad_enabled(False)\n",
    "from torch import *\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import time #used for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readme pls :)  \n",
    "All layers follow the same principle:  \n",
    "* **forward**: return the layer's output if given some input. Additionally, Linear layers will store the inputted data to do the backward pass.\n",
    "* **backward**: compute the layer's gradients given the successor layers' gradient. Linear layer will also store the next layer gradient to do its update step.\n",
    "* **step**: given an step size eta, will update the Linear weights and biases accordingly. Do nothing on activation layers.\n",
    "* **param** (utility): return layer's parameter in a list. Activation layers return an empty list while Linear layers return its weight and biases parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunc(object):\n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "    \n",
    "    def backward(self, *output_grad):\n",
    "        pass\n",
    "    \n",
    "    def requires_grad(self):\n",
    "        return False\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.x= 0\n",
    "    \n",
    "    def param(self):\n",
    "        return []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(ActivationFunc):        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        super().forward(x)\n",
    "        return (x > 0) * x\n",
    "    \n",
    "    def backward(self, *output_grad):\n",
    "        \"\"\"first item of output_grad should be the derivative of the previous layer\"\"\"\n",
    "        dl_dr = (self.x > 0) * 1.0 * output_grad[0]\n",
    "        return dl_dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(ActivationFunc):\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        super().forward(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "    def backward(self, *output_grad):\n",
    "        \"\"\"first item of output_grad should be the derivative of the previous layer\"\"\"\n",
    "        dl_ds = (self.sigmoid(self.x) * (1 - self.sigmoid(self.x))) * output_grad[0]\n",
    "        return dl_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU(ActivationFunc):        \n",
    "    \n",
    "    def lrelu(self, x, alpha=0.01):\n",
    "        return torch.max(alpha * x, x)\n",
    "    \n",
    "    def forward(self, x, alpha=0.01):\n",
    "        super().forward(x)\n",
    "        return self.lrelu(x)\n",
    "    \n",
    "    def backward(self, *output_grad,alpha=0.01):\n",
    "        \"\"\"first item of output_grad should be the derivative of the previous layer\"\"\"\n",
    "        inter = (self.x >=0) + (self.x < 0)*alpha\n",
    "        dl_dlr =  inter * output_grad[0]\n",
    "        return dl_dlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(ActivationFunc):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        super().forward(x)\n",
    "        return (2 / (1 + (-2*x).exp())) - 1\n",
    "    \n",
    "    def backward(self, *output_grad):\n",
    "        \"\"\"first item of output_grad should be the derivative of the previous layer\"\"\"\n",
    "        dl_dt = (1 - self.forward(self.x)**2) * output_grad[0]\n",
    "        return dl_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELU(ActivationFunc):        \n",
    "    \n",
    "    def forward(self, x, alpha=0.01):\n",
    "        #self.x = x\n",
    "        super().forward(x)\n",
    "        return (self.x >0)*x + (self.x <= 0)*alpha*(torch.exp(self.x) -1)\n",
    "    \n",
    "    def backward(self, *output_grad,alpha=0.01):\n",
    "        \"\"\"first item of output_grad should be the derivative of the previous layer\"\"\"\n",
    "        inter = (self.x >0) + (self.x <= 0)*alpha*torch.exp(self.x)\n",
    "        dl_del =  inter * output_grad[0]\n",
    "        return dl_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(ActivationFunc):\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        expo = torch.exp(x - torch.max(x))\n",
    "        return expo /torch.sum(expo,0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        super().forward(x)\n",
    "        #self.x = x\n",
    "        return self.softmax(x)\n",
    "    \n",
    "    def backward(self, *output_grad):\n",
    "        \"\"\"first item of output_grad should be the derivative of the previous layer\"\"\"\n",
    "        \n",
    "        s = self.softmax(self.x)\n",
    "        dl_dsm = s * (1 - s)\n",
    "        dl_dsm = dl_dsm * output_grad[0]\n",
    "        return dl_dsm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    \n",
    "    def __init__(self, in_size, out_size, init = None):\n",
    "        if init is None:\n",
    "            std = 1\n",
    "        elif init is 'Xavier':\n",
    "            std = math.sqrt(1.0 / (in_size + out_size))\n",
    "        elif init is 'He':\n",
    "            std = math.sqrt(2.0 / in_size)\n",
    "            \n",
    "        self.weights = torch.empty(in_size, out_size).normal_(std = std)\n",
    "        self.bias = Tensor(1, out_size).normal_()\n",
    "        self.x = 0\n",
    "        self.dx1 = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        #print('Forward Linear', (x @ self.weights) + self.bias)\n",
    "        return (x @ self.weights) + self.bias\n",
    "    \n",
    "    def backward(self, *output_grad):\n",
    "        #print('Backward Linear')\n",
    "        \"\"\"first item of output_grad should be the derivative of the previous layer\"\"\"\n",
    "        self.dx1 = output_grad[0]\n",
    "        self.dw = self.x.t() @ self.dx1\n",
    "        #print(self.db.shape)\n",
    "        self.db = (self.dx1).sum(axis=0)\n",
    "        #print(self.db.shape)\n",
    "        return output_grad[0] @ self.weights.t()\n",
    "    \n",
    "    def grad(self):\n",
    "        dw = self.x.t() @ self.dx1\n",
    "        db = self.dx1\n",
    "        return [dw, db]\n",
    "    \n",
    "    def step(self, eta):\n",
    "        #print(self.weights)\n",
    "        dw = self.x.t() @ self.dx1\n",
    "        db = self.dx1\n",
    "        self.weights.add_(-eta * self.dw)\n",
    "\n",
    "        #if len(db.shape) > 1:\n",
    "            #self.bias.add_(-eta * db.sum(axis=0))\n",
    "        #else:\n",
    "            #print('b')\n",
    "        self.bias.add_(-eta * self.db)\n",
    "        return\n",
    "    \n",
    "    def requires_grad(self):\n",
    "        return True\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.dw =0\n",
    "        self.db = 0\n",
    "        \n",
    "    def param(self):\n",
    "        return [[self.weights, self.dw], [self.bias, self.db]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(object):\n",
    "    \n",
    "    def __init__(self, *layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for layer in self.layers:\n",
    "            y = layer.forward(y)        \n",
    "        return y\n",
    "    \n",
    "    def backward(self, *output_grad):\n",
    "        d_list = [output_grad[0]]\n",
    "        \n",
    "        # We iterate from last layer to first layer\n",
    "        # Each layer's gradient is then prepended to the gradient list\n",
    "        for layer in self.layers[::-1]:\n",
    "            d_list.insert(0, layer.backward(*d_list))            \n",
    "        return d_list\n",
    "    \n",
    "    def step(self, eta):\n",
    "        for layer in self.layers:\n",
    "            if layer.requires_grad():\n",
    "                layer.step(eta)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grad()\n",
    "            \n",
    "    def param(self):\n",
    "        return [layer.param() for layer in self.layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "        a,_ = torch.max(x, dim = 1)\n",
    "        expo = torch.exp(x - a.view(-1,1))\n",
    "        #print(expo.shape)\n",
    "        #print(torch.sum(expo, dim = 1).shape)\n",
    "        return expo /torch.sum(expo, dim = 1).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    def loss(self,predicted,target):\n",
    "        pass\n",
    "    def dloss(self,predicted,target):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Loss):\n",
    "    def loss(self,predicted, target):\n",
    "        return ((predicted - target)**2).mean()\n",
    "    \n",
    "    def dloss(self,predicted, target):\n",
    "        return 2 * (predicted - target)/(len(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Loss):\n",
    "    \n",
    "    def loss(self,predicted, target):\n",
    "        a,_ = torch.max(predicted, dim = 1)\n",
    "\n",
    "        #loss = - (predicted*target).sum(dim=1) + torch.log(predicted.exp().sum(dim = 1))\n",
    "        loss = - (predicted*target).sum(dim = 1)\n",
    "        loss += a\n",
    "        loss += torch.log((predicted - a.view(-1,1)).exp().sum(dim=1))        \n",
    "        return loss.mean(dim = 0)\n",
    "    \n",
    "    def dloss(self,predicted, target):\n",
    "        #print((predicted == target)*1.0)\n",
    "        dloss = softmax(predicted) - target\n",
    "        return dloss/len(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, eta, model):\n",
    "        self.eta = eta\n",
    "        self.model = model\n",
    "        \n",
    "    def step(self):\n",
    "        pass\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \n",
    "    def __init__(self, eta, model, weight_decay = 0):\n",
    "        super().__init__(eta, model)\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "    def step (self):\n",
    "        for layer in self.model.layers:\n",
    "            if layer.requires_grad():\n",
    "                for param in layer.param():\n",
    "                    param[0].add_(-self.eta * param[1] - self.weight_decay * param[0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_ratio(predicted, classes):\n",
    "    #predicted_classes = (predicted > 0.5) * 1\n",
    "    _, predicted_classes = torch.max(predicted, 1)\n",
    "    _, labels = torch.max(classes, 1)\n",
    "    e = (predicted_classes - labels).float().abs().mean()\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(nb):\n",
    "    train_input = torch.rand(nb,2)\n",
    "    train_target = torch.empty(nb, dtype=torch.long)\n",
    "    train_target = train_input.sub(0.5).pow(2).sum(1).sub(1 / (2*math.pi)).sign().add(1).div(2).float().view(-1,1)\n",
    "    return train_input, train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot_labels(target):\n",
    "    tmp = torch.Tensor().new_zeros(target.size(0), int(target.max().item()) + 1)\n",
    "    tmp.scatter_(1, target.view(-1, 1).long(), 1.0)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target = generate_data(1000)\n",
    "test_input, test_target = generate_data(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = convert_to_one_hot_labels(train_target)\n",
    "test_target = convert_to_one_hot_labels(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size=100, eta=0.01, reg = 0.001, iterations=1000):\n",
    "    \n",
    "    optimizer = SGD(eta = eta, model = model, weight_decay = reg)\n",
    "    training_loss = torch.empty((iterations,1)) #Training loss over epochs\n",
    "    training_error = torch.empty((iterations,1)) #Training error over epochs\n",
    "    criterion = MSELoss()\n",
    "    for i in range(iterations):\n",
    "        for inputs, targets in zip(train_input.split(mini_batch_size),\n",
    "                                  train_target.split(mini_batch_size)):\n",
    "        \n",
    "            predicted = model.forward(inputs)\n",
    "            loss = criterion.loss(predicted, targets)\n",
    "            e = compute_error_ratio(predicted, targets)\n",
    "        \n",
    "            dloss = criterion.dloss(predicted, targets)\n",
    "        \n",
    "            model.backward(dloss)\n",
    "            #model.step(eta)\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "        \n",
    "        #if i%100 == 0:\n",
    "            #print(\"Training Loss at {:04d}: {:3.3f} with {:02.2f}% of error\".format(i, loss, e*100), end = '\\r')\n",
    "        training_loss[i] = loss\n",
    "        training_error[i] = e\n",
    "        print(i, loss.item(), end = '\\r')\n",
    "\n",
    "    return training_loss, training_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.normal_(m.weight)\n",
    "        m.bias.data.normal_()\n",
    "\n",
    "def train_model_torch(model, train_input, train_target, mini_batch_size=100, eta=0.01, reg = 0.001, iterations=1000):\n",
    "    \n",
    "    torch.set_grad_enabled(True)\n",
    "    model.apply(init_weights)\n",
    "    optimizer = SGD(eta = eta, model = model, weight_decay = reg)\n",
    "    training_loss = torch.empty((iterations,1)) #Training loss over epochs\n",
    "    training_error = torch.empty((iterations,1)) #Training error over epochs\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay = reg)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            predicted = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(predicted, train_target.narrow(0, b, mini_batch_size))\n",
    "            e = compute_error_ratio(predicted, train_target.narrow(0, b, mini_batch_size))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #if i%100 == 0:\n",
    "            #print(\"Training Loss at {:04d}: {:3.3f} with {:02.2f}% of error\".format(i, loss, e*100), end = '\\r')\n",
    "        training_loss[i] = loss\n",
    "        training_error[i] = e\n",
    "        print(i, loss.item(), end = '\\r')\n",
    "        \n",
    "    torch.set_grad_enabled(False)\n",
    "    return training_loss, training_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(rounds=10, iterations = 1000, is_torch = False, lr = 0.01, reg = 0.001):\n",
    "\n",
    "    test_error = torch.empty((rounds,1))\n",
    "    train_error = torch.empty((rounds,1))\n",
    "    training_loss = torch.empty((rounds,iterations))\n",
    "    for k in range(rounds):\n",
    "        model = create_model(is_torch)\n",
    "        \n",
    "        if is_torch: model.apply(init_weights)\n",
    "        num_samples = 1000\n",
    "        \n",
    "        #Generate data\n",
    "        train_input, train_target = generate_data(1000)\n",
    "        test_input, test_target = generate_data(1000)\n",
    "        \n",
    "        train_target = convert_to_one_hot_labels(train_target)\n",
    "        test_target = convert_to_one_hot_labels(test_target)\n",
    "        \n",
    "        if is_torch:\n",
    "            loss, _ = train_model_torch(model, train_input, train_target, iterations=iterations, eta = lr, reg = reg)\n",
    "            predicted_test = model(test_input)\n",
    "            predicted_train = model(train_input)\n",
    "        else:\n",
    "            loss,_ = train_model(model, train_input, train_target, iterations = iterations, reg = reg)\n",
    "            predicted_test = model.forward(test_input)\n",
    "            predicted_train = model.forward(train_input)\n",
    "        \n",
    "        \n",
    "        err_test = compute_error_ratio(predicted_test, test_target)\n",
    "        err_train = compute_error_ratio(predicted_train, train_target)\n",
    "        \n",
    "        #print(loss.shape)\n",
    "        training_loss[k] = loss.t()\n",
    "        train_error[k] = err_train\n",
    "        test_error[k] = err_test\n",
    "        \n",
    "    return (torch.mean(test_error).item(), torch.std(test_error).item(), torch.mean(train_error).item(), torch.std(train_error).item(), torch.mean(training_loss, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(torch, nb_hidden = 25):\n",
    "    if torch :\n",
    "        return nn.Sequential(nn.Linear(2,nb_hidden), nn.ReLU(), nn.Linear(nb_hidden,nb_hidden), nn.Tanh(), \n",
    "                           nn.Linear(nb_hidden,nb_hidden), nn.Sigmoid(), nn.Linear(nb_hidden,2))\n",
    "    else:\n",
    "        return Sequential(Linear(2,nb_hidden), ReLU(), Linear(nb_hidden,nb_hidden), Tanh(),\n",
    "                          Linear(nb_hidden,nb_hidden), Sigmoid(), Linear(nb_hidden,2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom framework with 10 rounds in 16.417131547099416:111486435 0.03806956112384796 0.037609148770570755 0.048460718244314194 0.04135759919881821 0.02793887071311474 0.04429428651928902 0.04339282587170601\n",
      "   training mean accuracy: 0.978\n",
      "   training std accuracy: 0.006\n",
      "   test mean accuracy: 0.975\n",
      "   test stdv accuracy: 0.007211\n",
      "\n",
      "999 0.032125134021043784 0.05139513313770294\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch framework with 10 rounds in 12.02842227930014:\n",
      "   training mean accuracy: 0.979\n",
      "   training std accuracy: 0.005\n",
      "   test mean accuracy: 0.972\n",
      "   test stdv accuracy: 0.007483\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_model = {\n",
    "    \"Custom framework\" : {\"torch\" : False},\n",
    "    \"Pytorch framework\" : {\"torch\" : True},\n",
    "}\n",
    "rounds= 10\n",
    "plt.figure()\n",
    "for key in dict_model.keys():\n",
    "    is_torch = dict_model[key][\"torch\"]\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    mean , std, mean_train, std_train, loss = evaluate_model(rounds, is_torch = is_torch, reg = 0)\n",
    "    avg_time = (time.perf_counter() - start) / rounds\n",
    "    mean = 1- mean\n",
    "    mean_train = 1 - mean_train\n",
    "    plt.plot(loss, label = \"{}\".format(key))\n",
    "    print(\"{} with {} rounds in {}:\".format(key, rounds, avg_time))\n",
    "    print(\"   training mean accuracy: {:.3f}\".format(mean_train))\n",
    "    print(\"   training std accuracy: {:.3f}\".format(std_train))\n",
    "    print(\"   test mean accuracy: {:.3f}\".format(mean))\n",
    "    print(\"   test stdv accuracy: {:.6f}\".format(std))\n",
    "    print()\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training loss')\n",
    "plt.savefig(\"comparison_with_mseloss.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
